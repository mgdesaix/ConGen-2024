---
title: "WGSassign basics"
author: "Matt DeSaix"
date: "2024-08-21"
output: rmarkdown::github_document
---

## Introduction to WGSassign

### Leave-one-out Assignment

Let's start using `WGSassign` with a subset of American Redstart data (a Neotropical migratory songbird), stored with the prefix `amre.western.*` (directory `./data/western/`. This dataset has 47 individuals from two populations on the Western portion of the breeding ground. These data are published in <https://doi.org/10.1111/mec.17137>, and Figure 1 of the paper shows the population structure.  We have a `.beagle.gz` file and a tab-delimited text file (`.wgsassign.ids.txt`) with two columns: individual and population IDs (BR = Basin Rockies; WB = Western Boreal). The pairwise Fst between these populations is around `0.006`...let's see how well we can assign these individuals back to their population!

The two files described above are the only files needed to initially perform leave-one-out cross-validation assignment and calculate the effective sample sizes (of the populations and individuals). **Importantly, the ID text file for WGSassign must have individuals in the same order as in the Beagle file!**

Let's run our first command in `WGSassign` from the Terminal. We'll load a Conda environment (provided with the installation of WGSassign containing the dependencies) and create an out directory. We'll specify options below to calculate reference allele frequencies (`--get_reference_af`), calculate effective sample sizes (`--ne_obs`), and perform leave-one-out cross-validation (`--loo`):

```sh
conda activate WGSassign
# change file paths as needed
beagle=./data/western/amre.western.beagle.gz
popIDs=./data/western/amre.western.wgsassign.ids.txt
outdir=./out/western
mkdir -p ${outdir}
out=${outdir}/amre.western.reference

WGSassign --beagle ${beagle} --pop_af_IDs ${popIDs} --get_reference_af --ne_obs --loo --out ${out}
```

which outputs to screen:

```sh
## WGSassign
## Matt DeSaix.
## Using 1 thread(s).
## 
## Parsing Beagle file.
## Loaded 99999 sites and 47 individuals.
## Parsing reference population ID file.
## EM (MAF) converged at iteration: 11
## EM (MAF) converged at iteration: 14
## Saved reference population allele frequencies as ./out/western/amre.western.reference.pop_af.npy (Binary - np.float32)
## 
## Column order of populations is: ['BR' 'WB']
## Saved reference population names as ./out/western/amre.western.reference.pop_names.txt (String: Order of pops for .pop_af.npy, .ne_obs.npy, and fisher_obs.npy files)
...
```

The output is verbose and provides some handy information - such as checking there's the expected number of sites and individuals in the file you provide. The `EM (MAF) converged at iteration` note comes up each time the population allele frequency (minor) is calculated. It occurs twice at the beginning (one for each pop) and also occurs during LOO as the allele frequency for a given population has to be recalculated without the individual that is being assigned...in our example, this occurred 47 times. This is the slowest step, but fortunately this is fast in `WGSassign` and can be sped up by increasing the number of threads used for parallelization (using the `--threads` argument). 

Let's switch into R and look at the file with the log likelihoods of assignment (`*.pop_like_LOO.txt`):

```{r, message = FALSE, warning=FALSE}
library(tidyverse)
# read in initial IDs file for individuals
western.ids <- read_table2("./data/western/amre.western.wgsassign.ids.txt",
                          col_names = c("Sample", "Breeding_pop"))
# read in wgsassign output
# population names
western.pops <- readLines("./out/western/amre.western.reference.pop_names.txt")
# LOO assignment log likelihoods
western.loo <- cbind(western.ids, read_table2("./out/western/amre.western.reference.pop_like_LOO.txt",
                          col_names = western.pops))
```

```r
head(western.loo)
```

```{r, echo = FALSE}
knitr::kable(head(western.loo))
```

Note that the `*.pop_like_LOO.txt` does not have the sample names, but is in the same order as the initial file we provided to `WGSassign` so we `cbind()` em.

We then want to convert the log likelihoods to posterior probabilities by exponentiating them and dividing by the sum. To deal with underflow issues of values getting too small for the computer when we exponentiate them, we'll also subtract the max value.

Many ways to do this, but below I pivot the table so I can easily group by the Sample IDs:

```{r}
western.loo.probs <- western.loo %>%
  pivot_longer(cols = BR:WB, # pivot by the assigned populations
               names_to = "AssignedPop",
               values_to = "AssignedLike") %>%
  group_by(Sample) %>%
  mutate(AssignedProb = round(exp(AssignedLike - max(AssignedLike)) / sum(exp(AssignedLike - max(AssignedLike))),2 )) # summarize posterior probabilities, grouped by individual
```

```r
head(western.loo.probs)
```

```{r, echo = FALSE}
knitr::kable(head(western.loo.probs))
```

Ideally, we'd capture some uncertainty in the assignments with the probabilities - but it is interesting to note that they are all 1 and 0, even when wrong! We'll get into this later. For now, we just want to filter out the population the individual is assigned to (`AssignedPop`) and determine the accuracy of assignment back to their known population (`Breeding_pop`).

```{r}
western.loo.summary <- western.loo.probs %>%
  filter(AssignedLike == max(AssignedLike)) %>% # keep only the maximum likelihood
  mutate(Correct = ifelse(Breeding_pop == AssignedPop, 1, 0)) %>% # create binary column of correct assignment
  group_by(Breeding_pop) %>%
  summarize(Accuracy = sum(Correct)/n()) # summarize accuracy by population
```

```r
western.loo.summary
```

```{r, echo = FALSE}
knitr::kable(western.loo.summary)
```

Well, mixed success. All individuals from Basin Rockies are accurately assigned back to the population, but only 65% of Western Boreal individuals are correctly assigned.

Let's take a look at the effective sample sizes of these populations

### Evaluating effective sample size

```{r, message=FALSE, warning=FALSE}
western.ess <- read_table2("./out/western/amre.western.reference.ne_obs.txt")
```

```r
western.ess
```

```{r, echo = FALSE}
knitr::kable(western.ess)
```

Basin Rockies has a notably higher effective sample size than Western Boreal and this may be the root of the problem - unbalanced sequencing depths and number of individuals between our populations. What can we do about this? Well, the population effective sample is determined by the sum of each individual's effective sample size in that population, and this individual-level effective sample size is also output by `WGSassign`. Let's take a look in R:

```{r, warning = FALSE, message = FALSE}
western.ess.ind <- western.ids %>% 
  cbind(read_table2("./out/western/amre.western.reference.ne_ind.txt",
                    col_names = "ess"))
```

```r
head(western.ess.ind)
```

```{r, echo = FALSE}
knitr::kable(head(western.ess.ind))
```

Let's see if there's any variation in individual effective sample between the populations - this would occur if there's differences in sequencing depth, which could arise from differences in sample types, sample age, etc.

```{r}
western.ess.ind.summary <- western.ess.ind %>%
  group_by(Breeding_pop) %>%
  summarize(N = n(),
            Mean_ess = mean(ess), # get mean effective sample size by group
            Max_ess = max(ess),
            Min_ess = min(ess))
```

```r
western.ess.ind.summary
```

```{r, echo = FALSE}
knitr::kable(western.ess.ind.summary)
```

The table above shows us that there are more individuals in the Basin Rockies group than Western Boreal (27 vs. 20), but also that Basin Rockies individuals tend to have higher effective sample sizes. What we want to do then is to remove individuals from the Basin Rockies population to get the population-level effective sample sizes more equal between the two, and then re-perform LOO assignment to evaluate accuracy. To do that, we need to identify the individuals we want to keep as part of the reference group for LOO, cut the Beagle file to those individuals and re-run WGSassign on the new Beagle file.

To do that, let's say we want to use 20 individuals per population. We can take our IDs file and subset it to that number of individuals:

```{r}
balanced.western.ids <- western.ids %>%
  group_by(Breeding_pop) %>%
  slice_head(n = 20) # cut to 20 individuals per group
```

We can now write this file out in the `WGSassign` format of the reference population ID file (i.e. tab-delimited, no header, first column = individual ID, second column = population ID)

```r
write_delim(balanced.western.ids, "./data/western/balanced.western.wgsassign.ids.txt", delim = "\t", col_names = FALSE)
```

Next we need to subset the Beagle file to these individuals. I provide a Bash script that uses `awk` to do this (`subset-beagle-ind.sh` in the `./data/scripts/` directory) - it takes as input the following required arguments

-  `-i`  single column file of individual IDs to be kept
-  `-b`  Beagle file
-  `-o`  outname of new Beagle file

********************

*Note:* You made need to give yourself executable authorization for the script, which can be run on the Terminal on the ConGen server:

```sh
chmod +x ./data/scripts/subset-beagle-ind.sh
```

Also, on my Mac, I sometimes get errors related to process substitution not occurring properly. This doesn't occur for me though on the Linux servers I've used, but this code uses process substitution and locally on my Mac I have to set:

```sh
set +o posix
```

For now too, I've also added "Mac" versions of the scripts for running these locally.

********************

Using the balanced western IDs file we created above, we can then subset the full Beagle file. The code below cuts the first column from the `balanced.western.wgsassign.ids.txt` to input the individuals we want to the script, followed by providing the Beagle file

```sh
individuals=./data/western/balanced.western.wgsassign.ids.txt
beagle=./data/western/amre.western.beagle.gz
outname=./data/western/balanced.western.beagle.gz

./data/scripts/subset-beagle-ind.sh -i <(cut -f1 ${individuals}) -b ${beagle} -o ${outname}
```

Check the header of your new Beagle file, does it look proper? It should! Note that the individuals are now arranged in the header in the same order as they are in the `balanced.western.wgsassign.ids.txt` file you used to cut them with the `awk` script. 


Now we can perform the LOO assignment with the new Beagle file and using the `balanced.western.wgsassign.ids.txt` specifying the individuals. That will be part of the break-out session and before we get to that, let's go over how to perform standard assignment in WGSassign.



### Standard Assignment

The "standard" assignment test in `WGSassign` performs assignment on a Beagle file of individuals to a separate file of population allele frequencies. Notably, it is assumed that the individuals in the Beagle file were not used in the allele frequency calculations, in order to avoid assignment bias...i.e. increased assignment accuracy will be perceived when an individual's genotype (likelihood) is included in the population it is being assigned to. 

Let's look at this and see how much we bias our results by *not* doing LOO on the reference individuals. Using the reference population allele frequencies for the American Redstarts of the West that we created from our initial run of WGSassign, let's determine the assignment accuracy of these reference individuals with the standard test. Below we'll specify the Beagle file (same as before) and the allele frequency file (created previously), along with the `--get_pop_like` argument to get the likelihood of assignment from the standard assignment (in contrast to `--loo` which we used before). We'll change the output name to the prefix of `amre.western.assign.*` to avoid confusion with our other files.

********************
Note: the allele frequencies are produced in a Numpy file (`*.npy`). This is from Python (which WGSassign is coded in) as a fast and easy to load file for the calculations but if you want to look at it in your terminal you will need to use Python. We won't be getting into that here.

********************

```sh
# change file paths as needed
beagle=./data/western/amre.western.beagle.gz
af=./out/western/amre.western.reference.pop_af.npy
outdir=./out/western
out=${outdir}/amre.western.assign

WGSassign --beagle ${beagle} --pop_af_file ${af} --get_pop_like --out ${out}
```

```sh
## WGSassign
## Matt DeSaix.
## Using 1 thread(s).
##
## Parsing Beagle file.
## Loaded 99999 sites and 47 individuals.
## Parsing population allele frequency file.
## Calculating likelihood of population assignment
## 47 individuals to assign to 2 populations
## Saved population assignment log likelihoods as ./out/western/amre.western.assign.pop_like.txt (text)
```

Super fast not having to recalculate allele frequencies for assigning each individual! Why don't we do this all the time for assessing reference population assignment?? Let's see...

Let's load in the likelihoods of assignment and see what the assignment accuracy looks like. I use the same R code as above, just pipe a little more of it together:


```{r, message = FALSE, warning = FALSE}
western.assign <- cbind(western.ids, read_table2("./out/western/amre.western.assign.pop_like.txt",
                          col_names = western.pops))
```

```r
head(western.assign)
```

```{r, echo = FALSE}
knitr::kable(head(western.assign))
```


```{r}
western.assign.summary <- western.assign %>%
  pivot_longer(cols = BR:WB,
               names_to = "AssignedPop",
               values_to = "AssignedLike") %>%
  group_by(Sample) %>%
  mutate(AssignedProb = round(exp(AssignedLike - max(AssignedLike)) / sum(exp(AssignedLike - max(AssignedLike))),2 )) %>%
  filter(AssignedLike == max(AssignedLike)) %>%
  ungroup() %>%
  mutate(Correct = ifelse(Breeding_pop == AssignedPop, 1, 0)) %>%
  group_by(Breeding_pop) %>%
  summarize(Accuracy = sum(Correct)/n(),
            .groups = "drop")
```

```r
western.assign.summary
```

```{r, echo = FALSE}
knitr::kable(western.assign.summary)
```

Wow, 100% accuracy for both pops! In contrast, we had much lower assignment accuracy before for the Western Boreal group. Clearly, we upwardly bias the results when including the individual's genotype likelihood in the allele frequency calculation...and this is why we use leave-one-out cross-validation for determining our assignment accuracy! But, the standard assignment is precisely what we can use for any individuals that are *not* part of the reference allele frequencies.

## Break-out session (20 min.)

For the first break-out session we'll put together all of these topics to assess assignment accuracy with the American Redstarts of the West data set. In groups, you will 

1.  select individuals from the Basin Rockies and Western Boreal populations that will result balanced effective sample sizes between the two populations, we'll call these the "LOO reference" individuals

2.  Write a new `*.wgsassign.ids.txt` file for these individuals and use the `subset-beagle-ind.sh` script to create a new Beagle file of your "LOO reference" individuals.

3.  Perform LOO assignment in WGSassign on the new "LOO reference" Beagle file and calculate the effective sample sizes

4.  Write another `*.wgsassign.ids.txt` file for all the individuals *not* part of the "LOO reference" group, we'll call these individuals the "Test" group. Create a new Beagle file for the "Test" group.

5.  Perform standard assignment in WGSassign on the "Test" group, using the `LOO reference` population allele frequencies.

6.  Summarize the assignment accuracy of the LOO reference and test individuals in R; report the effective sample sizes of the two populations; and provide an explanation of how you chose individuals to remove in order to balance the effective sample sizes.

7.  Bonus: If you finish early, play around with the data. How small of balanced effective sample sizes can you have in the reference population to still get accurate assignment? If you cut the Beagle file to 10k SNPs do you still get accurate assignment? 

The files you'll need for this are:

-  `./data/western/amre.western.beagle.gz`
-  `./data/western/amre.western.wgsassign.ids.txt`
-  `./data/scripts/subset-beagle-ind.sh`
-  `./data/full.amre.beagle.meta.csv`

The first three files we have already worked with. The new "meta data" file provides sampling site names (column = `Site`) for the individuals.

The purpose of this file is that the samples we have in our small data set are from multiple sampling sites within the populations - when balancing out the reference populations it may be best to select individuals from the different sampling sites. It's a CSV file with a header so can be read in to R with `read_csv`. The meta data file also contains individuals that are not part of our Western group subset, so I'd read it in and use a join with the Western IDs file we were using in R:

```r
full.meta <- read_csv("./data/full.amre.beagle.meta.csv")

western.ids.meta <- western.ids %>%
  left_join(full.meta, by = "Sample")
```

We'll reconvene shortly and discuss these results as a group!

********************

These wraps up the basics of evaluating assignment accuracy with known source individuals, now onto the next section [Full WGSassign workflow: 03-more-wgsassign.md](./03-more-wgsassign.md)



